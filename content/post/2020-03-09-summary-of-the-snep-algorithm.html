---
title: Summary of the SNEP algorithm
author: Mauro Camara Escudero
date: '2020-03-09'
slug: summary-of-the-snep-algorithm
categories:
  - paper-summaries
  - expectation-propagation
tags:
  - snep
  - natural-gradient
  - expectation-propagation
  - mcmc
subtitle: 'A minimalist and practical introduction.'
summary: 'Concised summary of Stochastic Natural Gradient Expectation Propagation algorithm by Hasenclever et al.'
authors: []
lastmod: '2020-03-09T16:19:31Z'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: bibliography.bib
---




<!-- # The SNEP Algorithm in a Nutshell -->
<div id="context" class="section level1">
<h1>Context</h1>
<p>Prior is in the base exponential family
<span class="math display">\[
p_0(x) = \exp(\theta_0^\top s(x) - A(\theta_0))
\]</span>
Each node <span class="math inline">\(i=1, \ldots, n\)</span> has a partition of the data. Local likelihood <span class="math inline">\(\ell_i(x)\)</span> can be formed
<span class="math display">\[
\ell_i(x) =\sum_{c\in S_i} \log p(y_c \mid x)
\]</span>
Posterior can be written as extended exponential family
<span class="math display">\[
\widetilde{p}(x) \propto p_0(x) \exp\left(\sum_{i=1}^n \ell_i(x)\right) \propto \exp\left(\widetilde{\theta}^\top \widetilde{s}(x) - \widetilde{A}(\widetilde{\theta})\right)
\]</span>
where
<span class="math display">\[
\begin{align}\widetilde{s}(x) &amp;= [s(x), \ell_1(x), \ldots, \ell_n(x)]  &amp;&amp;\Longleftrightarrow \qquad (\phi(x), \Phi^1(x), \ldots, \Phi^{n}(x)) \\\widetilde{\theta} &amp;= [\theta_0, \boldsymbol{1}_n] &amp;&amp;\Longleftrightarrow \qquad (\theta, \widetilde{\theta}^1, \ldots, \widetilde{\theta}^n)\end{align}
\]</span></p>
</div>
<div id="aim" class="section level1">
<h1>Aim</h1>
<p>Want to learn the posterior distribution <span class="math inline">\(\widetilde{p}(x)\)</span>. Do this by finding the log-partition function and the mean parameters, which completely specify the posterior distribution. Both can be found altogether by solving the variational problem represented by the convex dual. This problem is difficult to solve, so we solve an approximated problem instead.</p>
</div>
<div id="variational-problem" class="section level1">
<h1>Variational Problem</h1>
<p>The approximated problem that we need to solve to find the (approximated) mean parameters and the (approximated) log-partition function is
<span class="math display">\[
\begin{alignat}{2}
    &amp;\!\max_{\mu_0, [\mu_i, \nu_i]_{i=1}^n}       &amp;\qquad&amp; \left\{\langle\mu_0, \theta_0\rangle + \sum_{i=1}^{n} \langle \nu_i, 1\rangle  - A^*(\mu_0) -\sum_{i=1}^{n} \beta_i[A^*(\mu_i, \nu_i) - A^*(\mu_i)]\right\}\\
    &amp;\text{s.t.} &amp;      &amp; (\mu_i, \nu_i)\in \mathcal{M}(s(x), \ell_i(x)) &amp;\forall i=1, \ldots, n\\
    &amp;            &amp;      &amp; \mu_i = \mu_0  &amp; \forall i=1, \ldots, n\\
    &amp;            &amp;      &amp; \mu_0\in \mathcal{M}(s(x))
\end{alignat}
\]</span>
Solving this gives an approximation to the posterior that belongs to the exponential family
<span class="math display">\[
\widetilde{p}(x)\propto \exp\left(\left\langle \theta_0 + \sum_{i=1}^n \lambda_i, s(x)\right\rangle\right)
\]</span>
Since exponential family is closed under multiplication (adding up the coefficients), we can interpret this approximation as follows. Each <span class="math inline">\(\lambda_i\)</span> is the natural parameter of an exponential family approximating the intractable likelihood term in site <span class="math inline">\(i\)</span> given by <span class="math inline">\(\exp(\ell_i(x))\)</span>.</p>
<!-- # Introduction -->
<!-- Computation of posterior in Bayesian model is intractable. Approximate schemes include -->
<!-- - Variational Inference [@ExpFamVarInf] -->
<!-- - Markov Chain Monte Carlo [@MCMCPractice] -->
<!-- - Sequential Monte Carlo [@SMCPractice] -->
<!-- SNEP builds on top of the Expectation Propagation method developed by @PosteriorSamplingMomentSharing, because the latter method was found to perform poorly on complex models such as Bayesian Deep Neural Networks. -->
<!-- >need to add diagram of algorithm -->
<!-- # Problem Set-up and Background -->
<!-- ### Prior, Likelihood and Posterior -->
<!-- **Prior** is in the base exponential family -->
<!-- $$ -->
<!-- p_0(x) = \exp(\theta_0^\top s(x) - A(\theta_0)) -->
<!-- $$ -->
<!-- And on each node $i=1, \ldots, n$ there is a **log likelihood term** -->
<!-- $$ -->
<!-- \ell_i(x) =\sum_{c\in S_i} \log p(y_c \mid x) -->
<!-- $$ -->
<!-- Then the **posterior** can be written as part of the _extended exponential family_  -->
<!-- $$ -->
<!-- \widetilde{p}(x) \propto p_0(x) \exp\left(\sum_{i=1}^n \ell_i(x)\right) \propto \exp\left(\widetilde{\theta}^\top \widetilde{s}(x) - \widetilde{A}(\widetilde{\theta})\right) -->
<!-- $$ -->
<!-- where -->
<!-- $$ -->
<!-- \begin{align}\widetilde{s}(x) &= [s(x), \ell_1(x), \ldots, \ell_n(x)]  &&\Longleftrightarrow \qquad (\phi(x), \Phi^1(x), \ldots, \Phi^{n}(x)) \\\widetilde{\theta} &= [\theta_0, \boldsymbol{1}_n] &&\Longleftrightarrow \qquad (\theta, \widetilde{\theta}^1, \ldots, \widetilde{\theta}^n)\end{align} -->
<!-- $$ -->
<!-- ### Extended and Augmented Variational Problem -->
<!-- The _extended_ variational problem to be solved is -->
<!-- $$ -->
<!-- \max_{\widetilde{\mu}\in\widetilde{\mathcal{M}}} \widetilde{\theta}^\top \widetilde{\mu} - \widetilde{A}^*(\widetilde{\mu}) -->
<!-- $$ -->
<!-- This can be **relaxed** by  -->
<!-- - optimizing over an _outer bound_ of $\widetilde{\mathcal{M}}$ and  -->
<!-- - approximating $-\widetilde{A}^*(\widetilde{\mu})$ with a _term-by-term approximation_  -->
<!-- Leading to the following **optimization problem** -->
<!-- $$ -->
<!-- \begin{alignat}{2} -->
<!--     &\!\max_{\mu_0, [\mu_i, \nu_i]_{i=1}^n}       &\qquad& \left\{\langle\mu_0, \theta_0\rangle + \sum_{i=1}^{n} \langle \nu_i, 1\rangle  - A^*(\mu_0) -\sum_{i=1}^{n} \beta_i[A^*(\mu_i, \nu_i) - A^*(\mu_i)]\right\}\\ -->
<!--     &\text{s.t.} &      & (\mu_i, \nu_i)\in \mathcal{M}(s(x), \ell_i(x)) &\forall i=1, \ldots, n\\ -->
<!--     &            &      & \mu_i = \mu_0  & \forall i=1, \ldots, n\\ -->
<!--     &            &      & \mu_0\in \mathcal{M}(s(x)) -->
<!-- \end{alignat} -->
<!-- $$ -->
<!-- ### Solving the Optimization Problem -->
<!-- This is solved by forming the **Lagrangian** for the equality conditions -->
<!-- $$ -->
<!-- L = \langle\mu_0, \theta_0\rangle + \sum_{i=1}^{n} \langle \nu_i, 1\rangle  - A^*(\mu_0) -\sum_{i=1}^{n} \beta_i[A^*(\mu_i, \nu_i) - A^*(\mu_i)] - \sum_{i=1}^n\langle\lambda_i, \mu_i-\mu_0\rangle -->
<!-- $$ -->
<!-- find its derivatives and setting them to zero. -->
<!-- $$ -->
<!-- \begin{align} -->
<!--    \nabla_{\mu_0}L  -->
<!--    &= \theta_0-\nabla_{\mu_0}A^*(\mu_0) + \sum_{i=1}^n \lambda_i  -->
<!--    &&\implies  \nabla_{\mu_0} A^*(\mu_0)  -->
<!--    =&&& \theta_0 + \sum_{i=1}^n \lambda_i \\ -->
<!--    \nabla_{\lambda_j} L  -->
<!--    &= \mu_0 - \mu_j  -->
<!--    &&\implies \mu_j  -->
<!--    =&&& \mu_0 \\ -->
<!--    \nabla_{\mu_j} L  -->
<!--    &= -\beta_j\nabla_{\mu_j}A^*(\mu_j, \nu_j) + \beta_j \nabla_{\mu_j}A^*(\mu_j) - \lambda_j  -->
<!--    &&\implies \nabla_{\mu_j} A^*(\mu_j, \nu_j)  -->
<!--    =&&& \theta_0  + \sum_{i=1}^n \lambda_i - \frac{\lambda_j}{\beta_j} \\ -->
<!--    \nabla_{\nu_j} L  -->
<!--    &= 1 - \beta_j\nabla_{\nu_j}A^*(\mu_j, \nu_j)  -->
<!--    &&\implies \nabla_{\nu_j} A^*(\mu_j, \nu_j)  -->
<!--    =&&& \frac{1}{\beta_j} -->
<!-- \end{align} -->
<!-- $$ -->
<!-- ### Interpreting the Solutions -->
<!-- **Posterior** can be approximated by -->
<!-- $$ -->
<!-- \widetilde{p}(x)\propto \exp\left(\left\langle \theta_0 + \sum_{i=1}^n \lambda_i, s(x)\right\rangle\right) -->
<!-- $$ -->
<!-- Basically each $\lambda_i$ is the natural parameter of an exponential family approximating the intractable likelihood term in site $i$ given by $\exp(\ell_i(x))$.  -->
<!-- The **tilted** distribution at site $i$ corresponds to the $(s(x), \ell_i(x))$-Exponential Family and thus is given by -->
<!-- $$ -->
<!-- \begin{align} -->
<!-- p_i(x) &\propto \exp\left(\left\langle \left[\theta_0 + \sum_{j=1}^n \lambda_j - \frac{\lambda_i}{\beta_i}, \frac{1}{\beta_i}\right] , [s(x), \ell_i(x)]\right\rangle\right) \\ -->
<!-- & \propto \exp\left(\left\langle \theta_0 + \sum_{j=1}^n \lambda_j - \frac{\lambda_i}{\beta_i}, s(x) \right\rangle + \left\langle \frac{1}{\beta_i}, \ell_i(x)\right\rangle\right) -->
<!-- \end{align} -->
<!-- $$ -->
<!-- # Mathematical Background -->
<!-- ### Natural Gradient Descent -->
<!-- - **Fisher Information Matrix**: Measures the curvature of $\ell(\theta\mid x)$. Often used as replacement of $H_{\ell(\theta\mid x)}$ in second order optimization methods. -->
<!--   - **Variance** of the score (gradient of log-likelihood) $F = \mathbb{E}_{\mathcal{L}(\theta\mid x)}\left[\nabla_\theta\ell(\theta\mid x)\,\nabla_\theta\ell(\theta\mid x)^\top\right]$ -->
<!--   - Negative expected **Hessian** for the log-likelihood $F = -\mathbb{E}_{\mathcal{L}(\theta\mid x)}\left[H_{\ell(\theta\mid x)}\right]$ -->
<!--   - Hessian (with respect to $\theta'$) of the **KL divergence** between $\mathcal{L}(\theta\mid x)$ and $\mathcal{L}(\theta'\mid x)$ evaluated at $\theta' = \theta$. -->
<!--   $$ -->
<!--   F = \nabla_{\theta'}\text{KL}(\mathcal{L}(\theta\mid x) \parallel \mathcal{L}(\theta'\mid x)\eval_{\theta' = \theta} -->
<!--   $$ -->
<!-- - **Steepest Descent**: Steepest direction minimizing $-\ell(\theta\mid x)$ is the solution of an optimization problem depending on the norm $\parallel \vd \parallel$. Different norms dive different solutions -->
<!-- $$ -->
<!-- \vd^* = \arg \min_{\vd\,:\, \parallel \vd \parallel = 1} -\nabla_\theta \ell(\theta \mid x)^\top \vd -->
<!-- $$ -->
<!-- - **Natural Gradient**: Use KL-divergence as a "metric" in the distribution space. Although asymmetric, as $\vd$ goes to zero, KL approximately symmetric. Steepest descent direction  $\vd^* = -F^{-1}\nabla_\theta \ell(\theta\mid x)$ is the solution of -->
<!-- $$ -->
<!-- \vd^* = \arg\min_{\vd \,:\, \text{KL}(\mathcal{L}(\theta) \parallel \mathcal{L}(\theta')) = c} -\nabla_\theta \ell(\theta \mid x)^\top \vd -->
<!-- $$ -->
</div>
<div id="bibliography" class="section level1">
<h1>Bibliography</h1>
</div>
