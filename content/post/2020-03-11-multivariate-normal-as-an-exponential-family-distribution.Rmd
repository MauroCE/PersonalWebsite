---
title: Multivariate Normal as an Exponential Family Distribution
author: Mauro Camara Escudero
date: '2020-03-11'
slug: multivariate-normal-as-an-exponential-family-distribution
categories:
  - statistics
tags:
  - multivariate-normal
  - multivariate-gaussian
  - exponential-family
subtitle: ''
summary: 'How to rewrite a Multivariate Normal distribution as a member of the Exponential Family.'
authors: []
lastmod: '2020-03-11T13:35:34Z'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\nc}[2]{\newcommand{#1}{#2}}
\nc{\vx}{\vect{x}}
\nc{\vmu}{\vect{\mu}}
\nc{\vSigma}{\vect{\Sigma}}
\nc{\vtheta}{\vect{\theta}}
\nc{\vX}{\vect{X}}
\newcommand{\Ebb}[1]{\mathbb{E}\left[#1\right]}

### Exponential Family of Distributions
A density $f(\vx)$ belongs to the exponential family of distributions if we can write it as
$$
f(\vx; \vtheta) = \exp\left\{\langle\vtheta, \phi(\vx)\rangle  - A(\vtheta)\right\}
$$
we call $\vtheta$ its natural parameters, while we call $\mathbb{E}_f[\phi(\vX)]$ its mean parameters.

### Multivariate Normal Distribution
A pdf $f$ is a multivariate normal distribution if
$$
f(\vx) = (2\pi)^{-\frac{d}{2}}\text{det}(\vSigma)^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}(\vx - \vmu)^\top \vSigma^{-1}(\vx - \vmu)\right\}
$$

This can be rearranged as
$$
f(\vx) = \exp\left\{\vx^\top\vSigma^{-1}\vmu -\frac{1}{2}\vx^\top\vSigma^{-1}\vx -\frac{1}{2}\left[d\log2\pi + \log|\vSigma| +\vmu^\top\vSigma^{-1}\vmu\right]\right\}
$$

### Frobenius Inner Product
Notice that we can write the second term as
$$
-\frac{1}{2}\vx^\top \vSigma^{-1}\vx = -\frac{1}{2}\sum_{k=1}^d \sum_{j=1}^d x_k\Sigma_{kj}^{-1}x_j
$$
similarly, the following expression can be written in the same way
$$
\begin{align}
\text{tr}\left[-\frac{1}{2}\vSigma^{-1}\vx\vx^\top\right] &= 
-\frac{1}{2}\text{tr}\left[
\begin{pmatrix}
\Sigma_{11}^{-1} & \cdots & \Sigma_{1d}^{-1} \\
\vdots & \ddots & \vdots \\
\Sigma_{d1}^{-1} & \cdots & \Sigma_{dd}^{-1} 
\end{pmatrix}
\begin{pmatrix}
x_1^2 & \cdots & x_1x_d\\
\vdots & \ddots & \vdots\\
x_dx_1 & \cdots & x_d^2
\end{pmatrix}
\right]\\
&= 
-\frac{1}{2}\text{tr}\left[
\begin{pmatrix}
\sum_{j=1}^d\Sigma_{1j}^{-1}x_jx_1 & \cdots & \sum_{j=1}\Sigma_{1j}^{-1}x_jx_d \\
 \vdots & \ddots & \vdots \\
\sum_{j=1}^d \Sigma_{dj}^{-1}x_jx_1 & \cdots & \sum_{j=1}^d \Sigma_{dj}^{-1}x_jx_d
\end{pmatrix}
\right]\\
&= -\frac{1}{2}\sum_{k=1}^d\sum_{j=1}^d x_{k}\Sigma_{kj}^{-1}x_j
\end{align}
$$
Now notice that this is nothing but the Frobenius inner product between two **real** and **symmetric** matrices (which can be written both in terms of a trace and in terms of $\text{vec}$-torizing operations)
$$
\begin{align}
\left\langle -\frac{1}{2}\vSigma^{-1}, \vx\vx^\top\right\rangle_F 
&= \text{tr}\left(-\frac{1}{2}\vSigma^{-1}\vx\vx^\top\right) \\
&= \text{vec}\left(-\frac{1}{2}\vSigma^{-1}\right)^\top\text{vec}\left(\vx\vx^\top\right)
\end{align}
$$
where the vectorized operation for an $n\times m$ matrix $A$ simply stacks the rows one at a time to create a $(nm)\times 1$ vector
$$
\text{vec}[A] = \text{vec}\left[\begin{pmatrix} a_{11} & \cdots & a_{1m} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nm}\end{pmatrix}\right] = \begin{pmatrix} a_{11} \\ \vdots  \\ a_{1m} \\ \vdots \\ a_{nm}\end{pmatrix}
$$

this allows us to write the pdf as
$$
f(\vx) = \exp\left\{\langle \vSigma^{-1}\vmu, \vx\rangle + \left\langle \text{vec}\left(-\frac{1}{2}\vSigma^{-1}\right), \text{vec}\left(\vx\vx^\top\right)\right\rangle -\frac{1}{2}\left[d\log2\pi + \log|\vSigma| +\vmu^\top\vSigma^{-1}\vmu\right]\right\}
$$

### Natural Parameters of a Multivariate Normal Distribution
Since $\vSigma^{-1}\vmu$ can be written as
$$
\vSigma^{-1}\vmu = 
\begin{pmatrix}
\sum_{j=1}^d \Sigma_{1j}^{-1}\mu_j \\
\vdots \\
\sum_{j=1}^d \Sigma_{dj}^{-1}\mu_j
\end{pmatrix}
$$
the natural parameters are given by
$$
\vtheta = 
\begin{pmatrix}
\vSigma^{-1}\vmu \\
\text{vec}\left[-\frac{1}{2}\vSigma^{-1}\right]
\end{pmatrix} 
= 
\begin{pmatrix}
\sum_{j=1}^d \Sigma_{1j}^{-1}\mu_j \\
\vdots \\
\sum_{j=1}^d \Sigma_{dj}^{-1}\mu_j \\
-\frac{1}{2}\Sigma_{11}^{-1} \\ \vdots \\ -\frac{1}{2}\Sigma_{1d}^{-1} \\ \vdots \\ -\frac{1}{2}\Sigma_{dd}^{-1}
\end{pmatrix}_{(d + d^2)\times 1}
$$

In a similar fashion, we can find the sufficient statistics as
$$
\phi(\vx) = \begin{pmatrix}
\vx \\
\text{vec}(\vx\vx^\top)
\end{pmatrix}
=
\begin{pmatrix}
x_1 \\
\vdots \\
x_d \\
x_1^2 \\ \vdots \\ x_1x_d \\ \vdots \\ x_d^2
\end{pmatrix}_{(d + d^2)\times 1}
$$

This gives the complete expression for the multivariate normal distribution as part of the Exponential Family of distributions

\begin{align}
f(\vx) &= \exp\left\{\langle \theta, \phi(\vx)\rangle - A(\vtheta)\right\} \\
&= \exp\left\{
  \left(\sum_{j=1}^d \Sigma_{1j}^{-1}\mu_j, \cdots, \sum_{j=1}^d \Sigma_{dj}^{-1}\mu_j,
  -\frac{1}{2}\Sigma_{11}^{-1}, \cdots, -\frac{1}{2}\Sigma_{1d}^{-1},  \cdots, -\frac{1}{2}\Sigma_{dd}^{-1}
  \right)^\top
  
  \begin{pmatrix}
  x_1 \\
  \vdots \\
  x_d \\
  x_1^2 \\ \vdots \\ x_1x_d \\ \vdots \\ x_d^2
  \end{pmatrix} -\frac{1}{2}\left[d\log2\pi + \log|\vSigma| +\vmu^\top\vSigma^{-1}\vmu\right]
  \right\}
\end{align}

### Mean Parameters of a Multivariate Normal Distribution
Remember that the expected value of a matrix or a vector is taken element-wise, so that if we were to compute 
$$
\begin{align}
(\mathbb{E}[\vx\vx^\top])_{ij} 
&= \mathbb{E}[(\vx\vx^\top)_{ij}] \\
&= \mathbb{E}[x_ix_j] \\
&= \text{cov}(x_i, x_j) + \mathbb{E}[x_i]\mathbb{E}[x_j] \\
&= \Sigma_{ij} + \mu_i \mu_j
\end{align}
$$
This means that taking the expected value of our vectorized matrix $\text{vec}(\vx\vx^\top)$ is equivalent to vectorizing the expected value of $\vx\vx^\top$
$$
\begin{align}
\mathbb{E}\left[\text{vec}(\vx\vx^\top)\right] 
&= \Ebb{
\begin{pmatrix}
x_1^2 \\
\vdots \\
x_1x_d \\
\vdots \\
x_d^2
\end{pmatrix}
} \\
&=
\begin{pmatrix}
\Ebb{x_1^2} \\
\vdots \\
\Ebb{x_1x_d} \\
\vdots \\
\Ebb{x_d^2}
\end{pmatrix} \\
&= 
\begin{pmatrix}
\Sigma_{11} + \mu_1^2 \\
\vdots \\
\Sigma_{1d} + \mu_1\mu_d \\
\vdots \\
\Sigma_{dd} + \mu_d^2
\end{pmatrix} \\
&= \text{vec}\left(\Ebb{\vx\vx^\top}\right)
\end{align}
$$
Finally, the mean parameters are given by
$$
\begin{align}
\mathbb{E}\left[\phi(\vx)\right] &= \Ebb{\begin{pmatrix}
\vx \\
\text{vec}(\vx\vx^\top)
\end{pmatrix}} \\
&= \begin{pmatrix}
\vmu \\
\text{vec}\left(\vSigma + \vmu\vmu^\top\right)
\end{pmatrix}
\end{align}
$$

<!-- ### Division of Multivariate Normal Distributions -->
<!-- Suppose we have two multivariate normal distributions with the same sufficient statistics -->
<!-- $$ -->
<!-- \exp\left\{\vtheta_1^\top \phi(\vx) - A(\vtheta_1)\right\} \qquad \text{and} \qquad \exp\left\{\vtheta_2^\top\phi(\vx) - A(\vtheta_2)\right\} -->
<!-- $$ -->
<!-- Dividing the two pdfs we obtain -->
<!-- $$ -->
<!-- \exp\left\{(\vtheta_1 - \vtheta_2)^\top \phi(\vx) - A'(\vtheta_1 - \vtheta_2)\right\} -->
<!-- $$ -->
<!-- In our case we have -->
<!-- $$ -->
<!-- \vtheta_1 = \begin{pmatrix} -->
<!-- \vSigma^{-1}_1\vmu_1 \\ -->
<!-- \text{vec}\left[-\frac{1}{2}\vSigma_1\right] -->
<!-- \end{pmatrix} -->
<!-- \qquad -->
<!-- \text{and}  -->
<!-- \qquad -->
<!-- \vtheta_2 = \begin{pmatrix} -->
<!-- \vSigma^{-1}_2\vmu_2 \\ -->
<!-- \text{vec}\left[-\frac{1}{2}\vSigma_2\right] -->
<!-- \end{pmatrix} -->
<!-- $$ -->
<!-- Subtracting them we get -->
<!-- $$ -->
<!-- \begin{align} -->
<!-- \vtheta_1 - \vtheta_2  -->
<!-- &=  -->
<!-- \begin{pmatrix} -->
<!-- \vSigma_1^{-1}\vmu_1 - \vSigma_2^{-1}\vmu_2 \\ -->
<!-- \text{vec}\left[-\frac{1}{2}\vSigma_1\right] - \text{vec}\left[-\frac{1}{2}\vSigma_2\right] -->
<!-- \end{pmatrix} \\ -->
<!-- &= \begin{pmatrix} -->
<!-- \vSigma_1^{-1}\vmu_1 - \vSigma_2^{-1}\vmu_2  \\ -->
<!-- -\frac{1}{2}\text{vec}\left[\vSigma_1 - \vSigma_2\right] -->
<!-- \end{pmatrix} -->
<!-- \end{align} -->
<!-- $$ -->
























