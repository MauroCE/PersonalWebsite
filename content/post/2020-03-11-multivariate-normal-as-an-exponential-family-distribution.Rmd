---
title: Multivariate Normal as an Exponential Family Distribution
author: Mauro Camara Escudero
date: '2020-03-11'
slug: multivariate-normal-as-an-exponential-family-distribution
categories:
  - statistics
tags:
  - multivariate-normal
  - multivariate-gaussian
  - exponential-family
subtitle: ''
summary: 'How to rewrite a Multivariate Normal distribution as a member of the Exponential Family.'
authors: []
lastmod: '2020-03-11T13:35:34Z'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\nc}[2]{\newcommand{#1}{#2}}
\nc{\vx}{\vect{x}}
\nc{\vmu}{\vect{\mu}}
\nc{\vSigma}{\vect{\Sigma}}
\nc{\vtheta}{\vect{\theta}}

### Exponential Family of Distributions
A density $f(\vx)$ belongs to the exponential family of distributions if we can write it as
$$
f(\vx; \vtheta) = \exp\left\{\langle\vtheta, \phi(\vx)\rangle  - A(\vtheta)\right\}
$$

### Multivariate Normal Distribution
A pdf $f$ is a multivariate normal distribution if
$$
f(\vx) = (2\pi)^{-\frac{d}{2}}\text{det}(\vSigma)^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}(\vx - \vmu)^\top \vSigma^{-1}(\vx - \vmu)\right\}
$$

This can be rearranged as
$$
f(\vx) = \exp\left\{\vx^\top\vSigma^{-1}\vmu -\frac{1}{2}\vx^\top\vSigma^{-1}\vx -\frac{1}{2}\left[d\log2\pi + \log|\vSigma| +\vmu^\top\vSigma^{-1}\vmu\right]\right\}
$$

### Frobenius Inner Product
Notice that we can write the second term as
$$
\vx^\top \vSigma^{-1}\vx = \sum_{k=1}^d \sum_{j=1}^d x_k\Sigma_{kj}x_j
$$
similarly, the following expression can be written in the same way
$$
\begin{align}
\text{tr}\left[\vSigma\vx\vx^\top\right] &= 
\text{tr}\left[
\begin{pmatrix}
\Sigma_{11} & \cdots & \Sigma_{1d} \\
\vdots & \ddots & \vdots \\
\Sigma_{d1} & \cdots & \Sigma_{dd} 
\end{pmatrix}
\begin{pmatrix}
x_1^2 & \cdots & x_1x_d\\
\vdots & \ddots & \vdots\\
x_dx_1 & \cdots & x_d^2
\end{pmatrix}
\right]\\
&= 
\text{tr}\left[
\begin{pmatrix}
\sum_{j=1}^d\Sigma_{1j}x_jx_1 & \cdots & \sum_{j=1}\Sigma_{1j}x_jx_d \\
 \vdots & \ddots & \vdots \\
\sum_{j=1}^d \Sigma_{dj}x_jx_1 & \cdots & \sum_{j=1}^d \Sigma_{dj}x_jx_d
\end{pmatrix}
\right] = \sum_{k=1}^d\sum_{j=1}^d x_{k}\Sigma_{kj}x_j
\end{align}
$$
Now notice that this is nothing but the Frobenius inner product between two **real** and **symmetric** matrices
$$
\langle \vSigma, \vx\vx^\top\rangle_F = \text{tr}(\vSigma\vx\vx^\top)
$$
which allows us to write the pdf as
$$
f(\vx) = \exp\left\{\langle \vSigma^{-1}\vmu, \vx\rangle + \left\langle -\frac{1}{2}\vSigma, \vx\vx^\top\right\rangle_F -\frac{1}{2}\left[d\log2\pi + \log|\vSigma| +\vmu^\top\vSigma^{-1}\vmu\right]\right\}
$$

### Natural Parameters and Mean Parameters
From the expression above we recognize
$$
\vtheta = 
\begin{pmatrix}
\vSigma^{-1}\vmu \\
-\frac{1}{2}\vSigma
\end{pmatrix} \qquad
\phi(\vx) = \begin{pmatrix}\vx \\ \vx\vx^\top
\end{pmatrix}
$$
and the mean parameters are
$$
\mathbb{E}\left[\phi(\vx)\right] = \mathbb{E}\left[\begin{pmatrix}\vx \\ \vx\vx^\top
\end{pmatrix}\right] = \begin{pmatrix}
\vmu \\
\vSigma + \vmu\vmu^\top
\end{pmatrix}
$$

### In Practice
We write
$$
\begin{align}
\vSigma^{-1}\vmu &=
\begin{pmatrix}
\Sigma_{11}^{-1} & \cdots & \Sigma_{1d}^{-1}\\
\vdots & \cdots & \vdots \\
\Sigma_{d1}^{-1} & \cdots & \Sigma_{dd}^{-1}
\end{pmatrix}
\begin{pmatrix}
\mu_1 \\
\vdots \\
\mu_d
\end{pmatrix}
= 
\begin{pmatrix}
\sum_{j=1}^d \Sigma_{1j}^{-1}\mu_j \\
\vdots \\
\sum_{j=1}^d \Sigma_{dj}^{-1}\mu_j
\end{pmatrix}_{d\times 1}
\end{align}
$$
In addition, we notice that for two real matrices $A$ and $B$ the Frobenious inner product can be written as
$$
\langle A, B\rangle_F = \text{vec}(A)^\top \text{vec}(B)
$$
Where the $\text{vec}$ operator simply stacks one row at a time to obtain a single vector of length $mn$.
$$
\text{vec}[A] = \text{vec}\left[\begin{pmatrix} a_{11} & \cdots & a_{1m} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nm}\end{pmatrix}\right] = \begin{pmatrix} a_{11} \\ \vdots  \\ a_{1m} \\ \vdots \\ a_{nm}\end{pmatrix}
$$

So we can write
$$
\text{vec}\left[-\frac{1}{2}\vSigma\right] = \begin{pmatrix}
-\frac{1}{2}\Sigma_{11} \\ \vdots \\ -\frac{1}{2}\Sigma_{1d} \\ \vdots \\ -\frac{1}{2}\Sigma_{dd}\\ 
\end{pmatrix}
$$
and 
$$
\text{vec}\left[\vx\vx^\top\right] = \text{vec}\left[
\begin{pmatrix}
x_1^2 & \cdots & x_1x_d \\
\vdots & \ddots & \vdots \\\
x_dx_1 & \cdots & x_d^2
\end{pmatrix}
\right] = \begin{pmatrix}
x_1^2 \\ \vdots \\ x_1x_d \\ \vdots \\ x_d^2
\end{pmatrix}
$$
Then we can write our natural parameters and our sufficient statistics more practically
$$
\vtheta = 
\begin{pmatrix}
\vSigma^{-1}\vmu \\
\text{vec}\left[-\frac{1}{2}\vSigma\right]
\end{pmatrix} 
= 
\begin{pmatrix}
\sum_{j=1}^d \Sigma_{1j}^{-1}\mu_j \\
\vdots \\
\sum_{j=1}^d \Sigma_{dj}^{-1}\mu_j \\
-\frac{1}{2}\Sigma_{11} \\ \vdots \\ -\frac{1}{2}\Sigma_{1d} \\ \vdots \\ -\frac{1}{2}\Sigma_{dd}
\end{pmatrix}_{(d + d^2)\times 1}
$$
and 
$$
\phi(\vx) = \begin{pmatrix}
\vx \\
\text{vec}(\vx\vx^\top)
\end{pmatrix}
=
\begin{pmatrix}
x_1 \\
\vdots \\
x_d \\
x_1^2 \\ \vdots \\ x_1x_d \\ \vdots \\ x_d^2
\end{pmatrix}_{(d + d^2)\times 1}
$$
So that
$$
\begin{align}
\langle \vtheta, \phi(\vx)\rangle 
&= 
\left\langle\begin{pmatrix}
\vSigma^{-1}\vmu \\
\text{vec}\left[-\frac{1}{2}\vSigma\right]
\end{pmatrix} ,  
\begin{pmatrix}
\vx \\
\text{vec}(\vx\vx^\top)
\end{pmatrix}\right\rangle \\
&= \left(\vSigma^{-1}\vmu, \text{vec}\left[-\frac{1}{2}\vSigma\right]\right)^\top  \begin{pmatrix}
\vx \\
\text{vec}(\vx\vx^\top)
\end{pmatrix} \\
&= \langle \vSigma^{-1}\vmu, \vx \rangle  + \left\langle \text{vec}\left[-\frac{1}{2}\vSigma\right], \text{vec}(\vx\vx^\top)\right\rangle \\
&= \langle \vSigma^{-1}\vmu, \vx \rangle + \left\langle -\frac{1}{2}\vSigma, \vx\vx^\top\right\rangle_F
\end{align}
$$

### At a Glance

\begin{align}
f(\vx) &= \exp\left\{\langle \theta, \phi(\vx)\rangle - A(\vtheta)\right\} \\
&= \exp\left\{
  \left(\sum_{j=1}^d \Sigma_{1j}^{-1}\mu_j, \cdots, \sum_{j=1}^d \Sigma_{dj}^{-1}\mu_j,
  -\frac{1}{2}\Sigma_{11}, \cdots, -\frac{1}{2}\Sigma_{1d},  \cdots, -\frac{1}{2}\Sigma_{dd}
  \right)^\top
  
  \begin{pmatrix}
  x_1 \\
  \vdots \\
  x_d \\
  x_1^2 \\ \vdots \\ x_1x_d \\ \vdots \\ x_d^2
  \end{pmatrix} -\frac{1}{2}\left[d\log2\pi + \log|\vSigma| +\vmu^\top\vSigma^{-1}\vmu\right]
  \right\}
\end{align}

### Division of Multivariate Normal Distributions
Suppose we have two multivariate normal distributions with the same sufficient statistics
$$
\exp\left\{\vtheta_1^\top \phi(\vx) - A(\vtheta_1)\right\} \qquad \text{and} \qquad \exp\left\{\vtheta_2^\top\phi(\vx) - A(\vtheta_2)\right\}
$$
Dividing the two pdfs we obtain
$$
\exp\left\{(\vtheta_1 - \vtheta_2)^\top \phi(\vx) - A'(\vtheta_1 - \vtheta_2)\right\}
$$
In our case we have
$$
\vtheta_1 = \begin{pmatrix}
\vSigma^{-1}_1\vmu_1 \\
\text{vec}\left[-\frac{1}{2}\vSigma_1\right]
\end{pmatrix}
\qquad
\text{and} 
\qquad
\vtheta_2 = \begin{pmatrix}
\vSigma^{-1}_2\vmu_2 \\
\text{vec}\left[-\frac{1}{2}\vSigma_2\right]
\end{pmatrix}
$$
Subtracting them we get
$$
\begin{align}
\vtheta_1 - \vtheta_2 
&= 
\begin{pmatrix}
\vSigma_1^{-1}\vmu_1 - \vSigma_2^{-1}\vmu_2 \\
\text{vec}\left[-\frac{1}{2}\vSigma_1\right] - \text{vec}\left[-\frac{1}{2}\vSigma_2\right]
\end{pmatrix} \\
&= \begin{pmatrix}
\vSigma_1^{-1}\vmu_1 - \vSigma_2^{-1}\vmu_2  \\
-\frac{1}{2}\text{vec}\left[\vSigma_1 - \vSigma_2\right]
\end{pmatrix}
\end{align}
$$
























